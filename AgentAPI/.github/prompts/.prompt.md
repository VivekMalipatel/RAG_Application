# Context

Your Working Directory will be /mainpool/Projects/RAG_Application/Application/AgentAPI/ and you will not care about any code outside this and all the outside code except Indexer API and Model Router API is depricated code base.

Langgraph solves many of the complexity in creating AI agents, However it still has lot of concepts to learn and implement to make an agent fully functional. langgraph has ocean of features and possibilities to make agents better and act properly and manage the agent properly. But They have lot of dependencies and concepts that are really hard to implement it perfectly.

The Main reason we are building this API is to abstract the Langgraph/ Agentic hassle that is required to build several agents to the maximum possible. We will abstract it to the level in a way that, the client just use this API, and do the below,
1. Define the agent
   a. Agent Name
   b. Agent's Role
   c. Agent's Instructions (System Prompt)
   d. Capabiltities that agent will have (tool calling, MCP server calling, Image/Video/Audio Input output, etc)
2. Create a set of Multi-Agent Pool:
   a. Define and Put Multiple agents in One pool and Within the pool each agent will have thier Withing the pool role
   b. If there are multiple agents in a pool, they should be for sure an orchestrator agent taht controls and validates all the agents.

So now They have their agents created, Now in their view, this created Agent/Agent pool will be an LLM. Now the API will also manage chat functionalities too. Here the tricky part will be for each agent they created there will be multiple chat threads. And There can be multiple Agents/ Multi Agent pools for the saem client. SO the API will manage all these chat and memeory management.

Here there will be four memeory hierarchies: (Long Term, Short Term and Episodic Memory)
1. Within Thread memeory : There should be a memory of what a all have been discussed in that chat.
2. Within Agent/Agent Pool Memeory : There should be a memory of all the chat that agent/agent pool has done. And If agent pool, each agent in the pool will have their own memeory and the whole pool will also have a meomory. If we have soemthing like subpools inside a pool, the same memory hierarchy applies. (Group by User)
3. Withing User memory : Each client has multiple users registered and each user can use Multiple Agents/Agent pool. So the Users memeory will have the user's memory thoughtout all the agents.
3. Within Client memory : Each Clinet has Multiple users and There should also be a memeory that is shared between users for a cleint.

In This way we will be making a proper Memory for reinforcement learning for the agent to adapt to its experiences and learn from past.(Reflection)

# Default Characteristics of every Agent

1. Every Agent should have chain of thoughts reasoning
2. Every Agen should follow ReAct (Reasoning + Act)
3. Every Agent should posses a feedback loop and contiunously learn from feedback and memory
4. Stateful -> Strictly rememeber the previoud interations
5. Every Agents should Divide large problems to small sub tasks

The _build_core_workflow should have fixed set of nodes that are default (as per the default chars of an agent) and the classses that inherits this base classs should be able to add more nodes and tools to this as per their requirement. Note : Here there should be a way to add edges after plan and before evaluate. And The Default Nodes should be reconfigurable (overridden with default or extend the default fucntionalty for ex. An Agent might have different way of perciveing thing or planning things, so they should be configurable but the base Agen should hev this default generic implementations).

# Agentic Patterns

1. Planner - Executor - Evaulator Patterns : One agent plans, other agent executes and otehr agent validtes the output

# Agent Loop

Percieve -> Plan -> Act -> Learn -> Repeat

# Data Types

All the Schemas will be Pydantic schemas (not state dictionaries of etc)

# Tools and Agent Capabilities

1. Web Search
2. Python Code execution
3. Knwoledge search via Indexer API (That have built with FAISS)
4. Act as Model Contect Protocol Clients to connect and use Multiple MCP servers.

# Utility Agents

We will have utility agents that are extremely specialised and capable of doing a particular operations. These agents will always be initialised and will be available to work with any other clinet created agent and will be added to them by default. They are:

1. Coder Agnet : Can take the task and write and execute code based on the task. (For example, if we need to get data from a CSV file, instead of reading teh data, coder agenr tcan use python pandas to execute the code and query with the file and give back the output)
2. Web Search Agent : Specialised in Searching the Web and getting the conent from the search webpasges and giving back.
3. Fact Check Agent : An Agent that verifies if the Known Universal Truths are well maintained in the whole operations.
4. Censor Board Agent : The continosuly monitors outputs of each agent and looks for content violations and NSFW content and not allowed words.

More will be added as per requirements.

# Human-in-the-loop

At any stage, the cleint should be able to interrupt the process and input and direct the agents properly. Agents should then learn their mistake for previodu steps and continue working propelry as per instructions.

# Parallelisation

Every Independent agents should be able to work in parallel

# RBAC

<Yet to Decide How to Implement it>

# Security

1. Input and Output Sanitization
<More yet to decide>

# Tech Stack

1. For all kind of memeories we will be using SQlite as of now and later will be shifing to Redis and PostgreSQL
2. Langgraph with ModelRouter API (an Open ai compatibale custom made api that we will use using with OpenAI() with custom base URL)
3. FastAPI

# Coding Style

1. It should be completelely modular, Object oriented, well logged
2. No Icons/emoji's in the code or logs
3. No Comments or class/methods descriptions should be written.

# Stack

## Memory

app/db/db.py will be a centralised session manager for Sqlite

This will be used to for literally everything i.e in place of redis, postgres, etc etc and for all the memory items in the planned hierarchy.Now we will build seperate handlers on top fo rthis based on the memory hierarchy and these handlers will use db.pyLater we will be replacing the db.py needed with Redis, Postgres, etc and replace the dependency in the handler.

## Agent Base Class

app/agnet/agent.py will be a base class for the agent. This class will abstract the Langgraph gent functionality and will be exposed to build sophesticated functionalities planned on top of it. This class basically will sit on Top of LLM and make the LLM a basic agent.

So, This will implement the default agent and the agent implemented here will have all the # Default Characteristics of every Agent

1. Every Agent should have chain of thoughts reasoning
2. Every Agen should follow ReAct (Reasoning + Act)
3. Every Agent should posses a feedback loop and contiunously learn from feedback and memory
4. Stateful -> Strictly rememeber the previous interations
5. Every Agents should Divide large problems to small sub tasks

By Default it will follow this pattern Planner - Executor - Evaulator Patterns : One agent plans, other agent executes and otehr agent validtes the output. Hence this will be its loop Percieve -> Plan -> Act -> Learn -> Repeat


### Plan:

Here we start with iteratively asking the LLM to plan the next step to solve the given prompt. We will be asking the LLm to provide just one next step. and append that step to teh next iteration prompt. In this was we will have a list of all the steps to tackle this problem (Query).

Liek once a Plan() object is geenrated and added to the plan list. We need to pass the query and that particualr step to the LLM and get reasoning for this particualr step. Thei reasoning can be an aswer for the plan if the plan step has a question , or a task if the plan is describing to do something, etc

So There should be a reasoning queue so that as soon as a plan is added via add_step methos, that will add the decription to the the queue. Now The Reasoning loop picks that up and get a structured output about the reasoning i.e solving the plan and add that to the reason list.

Both of these should run in parallel. So that as soon as a plan step is added, there should also be reasoning done in parallel.

At last the index of each plan in PlanandReason object will match the index of each reason. 

### act

So now once the _plan and reason has doen its job it will just pass the plan and reason objec tot _act

Now _act will act based on it. as of now Just asnwer the user based on the given planandreason data.

So in the act we will pass on all the planning and reasoning data to the LLM and properly get the output as an aswer tot he query. 

So as of nwo we need to make the _act to properly understadn the query, plan and the reason and posperly format the response in Markdown. Teh resposne should not miss any thing that is mentioned in the planandreason

# Core Design Principles

The system follows three architectural axioms derived from modern agent framework requirements:

Modular Cognitive Architecture: Implements Planner-Executor-Evaluator patterns through LangGraph's supervisor nodes

Hierarchical State Management: Combines LangGraph's built-in checkpointing with custom SQLite/Redis integration for multi-level memory

Parallel Execution Model: Utilizes LangGraph Swarm for concurrent agent operations with automatic load balancing

# Agent API Design

The API exposes endpoints through FastAPI with JWT authentication and rate limiting, while internally using LangGraph's functional API for workflow composition. A typical request flows through:

Authentication layer

Input sanitization module

Agent router (determines target agent/pool)

LangGraph runtime with Model Router API

Output validation and formatting

# Agent API Implementation

## Base Agent Configuration

All agents inherit from a customized ReActAgent class implementing five core capabilities:

Chain-of-thought reasoning through structured output parsing

Feedback loops with LangGraph's checkpointing API

State persistence using SQLite-backed memory stores

Automatic task decomposition via planner nodes

Parallel tool execution with conflict resolution

# Multi-Agent Pools

Agent pools implement supervisor patterns with three specialized node types:

Orchestrator: LangGraph Supervisor agent managing workflow state

Validator: Fact-checking agent using knowledge graphs

Censor: NSFW content filter with real-time moderation

text
graph LR
    O[Orchestrator] --> P[Planner]
    P --> E[Executor]
    E --> V[Validator]
    V --> C[Censor]
    C --> O

# Memory Management System

Four-Tier Memory Architecture
The system implements a hybrid memory model combining:

Episodic Memory: for conversation history

Semantic Memory: for knowledge retrieval

Procedural Memory: for frequent workflows

Meta Memory:  for agent configuration

Table 1: Memory Characteristics

Type	Retention	Access Pattern	
Episodic	30 days	Sequential	
Semantic	Permanent	Random	
Procedural	1 hour	LRU	
Meta	Permanent	Relational	

