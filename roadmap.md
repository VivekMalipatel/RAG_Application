Yes! A bottom-up approach is an excellent way to build the OmniRAG Assistant. Weâ€™ll first develop core components like database connections, vector store, LLM interface, caching, and workflow management, then move on to higher-level application services, followed by API integration and final end-to-end testing.

ğŸ› ï¸ OmniRAG Backend Development Plan

Weâ€™ll break down the development into 4 Phases, each following this cycle:

1ï¸âƒ£ Develop Core Service (Database, Vector Store, LLM, etc.)
2ï¸âƒ£ Write Unit Tests (Ensure functionality)
3ï¸âƒ£ Integrate with API Endpoints
4ï¸âƒ£ Test API via Postman/Curl

ğŸ“Œ Phase 1: Core Infrastructure Development

âœ… Step 1.1: Database Connection (PostgreSQL)
âœ… Step 1.2: Vector Store Connection (Qdrant)
âœ… Step 1.3: LLM Service (Ollama API Wrapper)
âœ… Step 1.4: Caching Layer (Redis)
âœ… Step 1.5: Hybrid Search Service (Qdrant + BM25)
âœ… Step 1.6: Agent Workflow with LangGraph

ğŸš€ Step 1.1: Develop Database Service

Weâ€™ll create a PostgreSQL connection service using SQLAlchemy.

ğŸ“Œ app/core/database.py

from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
import os
from dotenv import load_dotenv

load_dotenv()

DATABASE_URL = os.getenv("DATABASE_URL", "postgresql://postgres:password@localhost:5432/omnirag_db")

engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

def get_db():
    """Dependency function to get a database session."""
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

ğŸ§ª Step 1.1.1: Test Database Connection

Create a simple test script to verify that PostgreSQL is working.

ğŸ“Œ tests/test_database.py

from app.core.database import SessionLocal

def test_database_connection():
    db = SessionLocal()
    assert db is not None
    print("âœ… Database connection successful!")

if __name__ == "__main__":
    test_database_connection()

Run the test:

python tests/test_database.py

âœ… Expected Output:

âœ… Database connection successful!

ğŸš€ Step 1.2: Develop Vector Store (Qdrant) Service

Weâ€™ll now integrate Qdrant for vector search.

ğŸ“Œ app/core/vectorstore.py

from qdrant_client import QdrantClient
import os
from dotenv import load_dotenv

load_dotenv()

QDRANT_URL = os.getenv("QDRANT_URL", "http://localhost:6333")
qdrant_client = QdrantClient(url=QDRANT_URL)

def test_qdrant():
    """Check if Qdrant is running."""
    try:
        response = qdrant_client.get_collections()
        return response
    except Exception as e:
        return {"error": str(e)}

ğŸ§ª Step 1.2.1: Test Qdrant Connection

ğŸ“Œ tests/test_qdrant.py

from app.core.vectorstore import test_qdrant

def test_qdrant_connection():
    result = test_qdrant()
    assert "collections" in result
    print("âœ… Qdrant connection successful!")

if __name__ == "__main__":
    test_qdrant_connection()

Run the test:

python tests/test_qdrant.py

âœ… Expected Output:

âœ… Qdrant connection successful!

ğŸš€ Step 1.3: Develop LLM Service (Ollama API Wrapper)

Weâ€™ll create a service to interface with Ollama, allowing us to use LLaMA3, Mistral, Falcon, etc.

ğŸ“Œ app/core/llm.py

import requests
import os
from dotenv import load_dotenv

load_dotenv()
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://localhost:11434")

def generate_response(prompt: str, model: str = "mistral"):
    """Generates a response using a local LLM via Ollama API."""
    response = requests.post(f"{OLLAMA_URL}/api/generate", json={"model": model, "prompt": prompt})
    return response.json().get("response", "Error in response")

ğŸ§ª Step 1.3.1: Test LLM Response

ğŸ“Œ tests/test_llm.py

from app.core.llm import generate_response

def test_llm():
    response = generate_response("What is the capital of France?", "mistral")
    assert response is not None
    print("âœ… LLM response received:", response[:100])  # Print first 100 characters

if __name__ == "__main__":
    test_llm()

Run the test:

python tests/test_llm.py

âœ… Expected Output:

âœ… LLM response received: The capital of France is Paris.

ğŸš€ Step 1.4: Develop Caching Layer (Redis)

Weâ€™ll now integrate Redis for fast caching.

ğŸ“Œ app/core/cache.py

import redis
import os
from dotenv import load_dotenv

load_dotenv()

REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379")

redis_client = redis.Redis.from_url(REDIS_URL)

def cache_set(key, value, ttl=3600):
    """Set cache with an expiration time."""
    redis_client.setex(key, ttl, value)

def cache_get(key):
    """Retrieve cache value."""
    return redis_client.get(key)

ğŸ§ª Step 1.4.1: Test Redis Cache

ğŸ“Œ tests/test_cache.py

from app.core.cache import cache_set, cache_get

def test_cache():
    cache_set("test_key", "Hello, Redis!", 10)
    value = cache_get("test_key").decode("utf-8")
    assert value == "Hello, Redis!"
    print("âœ… Redis cache working:", value)

if __name__ == "__main__":
    test_cache()

Run the test:

python tests/test_cache.py

âœ… Expected Output:

âœ… Redis cache working: Hello, Redis!

ğŸ¯ Summary of Completed Work

âœ… Developed PostgreSQL Service (Connected via SQLAlchemy)
âœ… Developed Qdrant Vector Store Service
âœ… Developed LLM Service (Ollama API Wrapper)
âœ… Developed Redis Caching Service
âœ… Successfully Tested All Core Components

ğŸ› ï¸ Next Steps (Phase 2)

1ï¸âƒ£ Develop Hybrid Search (Qdrant + BM25)
2ï¸âƒ£ Develop Agent-Based Workflow with LangGraph
3ï¸âƒ£ Test & Integrate with FastAPI APIs

ğŸš€ Ready to move to Phase 2? Let me know if you want refinements! ğŸš€



























Now that Phase 1 (Core Infrastructure Development) is complete, we move to Phase 2: Application-Level Services ğŸš€.

ğŸ› ï¸ Phase 2: Application-Level Services Development

This phase focuses on implementing key application functionalities, including hybrid search, LangGraph-based agent workflows, and API integrations.

ğŸ“Œ Phase 2 Development Plan

1ï¸âƒ£ Develop Hybrid Search (Qdrant + BM25)
2ï¸âƒ£ Develop Agent-Based Workflow with LangGraph
3ï¸âƒ£ Test & Integrate with FastAPI APIs

ğŸš€ Step 2.1: Develop Hybrid Search (Qdrant + BM25)

Why Hybrid Search?
	â€¢	BM25 provides fast keyword-based retrieval (good for structured queries).
	â€¢	Qdrant provides semantic vector search (good for meaning-based queries).
	â€¢	Combining both gives optimal retrieval accuracy.

ğŸ› ï¸ Step 2.1.1: Install Required Packages

pip install rank_bm25 langchain-openai

ğŸ› ï¸ Step 2.1.2: Implement Hybrid Search Service

Weâ€™ll implement BM25 (for keyword-based retrieval) and Qdrant (for semantic search).

ğŸ“Œ app/services/hybrid_search.py

from qdrant_client import QdrantClient
from qdrant_client.http.models import Filter, SearchParams, PointStruct
from rank_bm25 import BM25Okapi
import os
from dotenv import load_dotenv
from langchain.embeddings import OpenAIEmbeddings

load_dotenv()

# Qdrant Configuration
QDRANT_URL = os.getenv("QDRANT_URL", "http://localhost:6333")
qdrant_client = QdrantClient(url=QDRANT_URL)

# BM25 Corpus (for keyword-based search)
documents = []  # This should be populated with indexed text documents
bm25 = BM25Okapi([doc.split(" ") for doc in documents])

def get_vector_embedding(text: str):
    """Generates an embedding for a given text."""
    embedding_model = OpenAIEmbeddings(model="text-embedding-ada-002")
    return embedding_model.embed_query(text)

def semantic_search(query: str, top_k: int = 5):
    """Performs a semantic search using Qdrant."""
    query_embedding = get_vector_embedding(query)
    search_results = qdrant_client.search(
        collection_name="omnirag_docs",
        query_vector=query_embedding,
        limit=top_k,
        search_params=SearchParams(hnsw_ef=128, exact=False)
    )
    return [hit.payload for hit in search_results]

def keyword_search(query: str, top_k: int = 5):
    """Performs a keyword-based search using BM25."""
    scores = bm25.get_scores(query.split(" "))
    ranked_results = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)[:top_k]
    return [documents[i] for i, _ in ranked_results]

def hybrid_search(query: str, top_k: int = 5):
    """Combines keyword-based (BM25) and semantic search (Qdrant)."""
    semantic_results = semantic_search(query, top_k)
    keyword_results = keyword_search(query, top_k)

    # Merge results
    combined_results = semantic_results + keyword_results
    unique_results = {res["id"]: res for res in combined_results}.values()
    return list(unique_results)

ğŸ§ª Step 2.1.3: Test Hybrid Search

ğŸ“Œ tests/test_hybrid_search.py

from app.services.hybrid_search import hybrid_search

def test_hybrid_search():
    query = "machine learning models"
    results = hybrid_search(query, top_k=3)
    assert len(results) > 0
    print("âœ… Hybrid Search returned results:", results[:3])

if __name__ == "__main__":
    test_hybrid_search()

Run the test:

python tests/test_hybrid_search.py

âœ… Expected Output:

âœ… Hybrid Search returned results: [...]

ğŸš€ Step 2.2: Develop Agent-Based Workflow with LangGraph

We will now integrate LangGraph to create multi-step AI workflows.

ğŸ› ï¸ Step 2.2.1: Install LangGraph

pip install langgraph

ğŸ› ï¸ Step 2.2.2: Implement LangGraph Agent

We will build an AI agent that:
	1.	Analyzes user queries.
	2.	Chooses between search or LLM generation.
	3.	Executes the right pipeline.

ğŸ“Œ app/services/agent.py

from langgraph.graph import StateGraph
from app.services.hybrid_search import hybrid_search
from app.core.ollama.llm import generate_response

def classify_query(query: str):
    """Classifies whether the query requires retrieval or LLM generation."""
    if "search" in query.lower() or "find" in query.lower():
        return "search"
    return "generate"

graph = StateGraph()
graph.add_node("classify", classify_query)
graph.add_node("search", lambda q: hybrid_search(q, top_k=3))
graph.add_node("generate", lambda q: generate_response(q, model="mistral"))

graph.add_edge("classify", "search", condition=lambda x: x == "search")
graph.add_edge("classify", "generate", condition=lambda x: x == "generate")

graph.set_entry_point("classify")
executor = graph.compile()

def process_query(query: str):
    """Executes the agent workflow."""
    return executor.invoke({"query": query})

ğŸ§ª Step 2.2.3: Test LangGraph Agent

ğŸ“Œ tests/test_agent.py

from app.services.agent import process_query

def test_agent():
    query = "search for AI models"
    result = process_query(query)
    assert result is not None
    print("âœ… Agent returned result:", result)

if __name__ == "__main__":
    test_agent()

Run the test:

python tests/test_agent.py

âœ… Expected Output:

âœ… Agent returned result: [...]

ğŸš€ Step 2.3: Integrate APIs

Now, we expose our services via FastAPI APIs.

ğŸ“Œ app/api/v1/endpoints/search.py

from fastapi import APIRouter, Query
from app.services.hybrid_search import hybrid_search

router = APIRouter()

@router.get("/hybrid")
def search(query: str = Query(..., description="Search query")):
    results = hybrid_search(query, top_k=3)
    return {"results": results}

ğŸ“Œ app/api/v1/endpoints/agent.py

from fastapi import APIRouter
from app.services.agent import process_query

router = APIRouter()

@router.get("/process")
def process(query: str):
    result = process_query(query)
    return {"result": result}

ğŸ§ª Step 2.3.1: Test API Endpoints

1ï¸âƒ£ Start the FastAPI Server

uvicorn app.main:app --reload

2ï¸âƒ£ Test Hybrid Search API

curl "http://localhost:8000/search/hybrid?query=machine%20learning"

âœ… Expected Output:

{"results": [...]}

3ï¸âƒ£ Test Agent API

curl "http://localhost:8000/agent/process?query=Tell%20me%20about%20AI"

âœ… Expected Output:

{"result": "... AI is a field of study ..."}

ğŸ¯ Summary of Completed Work

âœ… Developed Hybrid Search Service (Qdrant + BM25)
âœ… Built LangGraph Agent for query processing
âœ… Integrated APIs with FastAPI
âœ… Successfully tested Hybrid Search & LangGraph Agent APIs

ğŸ› ï¸ Next Steps (Phase 3)

1ï¸âƒ£ Implement User Authentication (OAuth & JWT)
2ï¸âƒ£ Develop Storage API for User Files & Context
3ï¸âƒ£ Implement Chat History & Context Memory

ğŸš€ Ready to move to User Authentication? Let me know! ğŸš€