# Context

We are starting to build a new API using Fast API This would be a File Indexer Microservice as part of a RAG system.

scope of this microservice is Take any file(PDF, Image, Video, Audio - all in multiple formats) or RAW Text (JSON, HTML,Markdown) or Link and embed that data and store in vector db and possible generate Knowledge graphs. It also is responsible for updating/storing and maintaining the incoming files's extracted content, metadata, source, etc etc in a SQL database (SQLite as of now but will later upgrade to postgres). Handler classes for qdrant and Neo4j is out of scope of this project. This API Takes the file and gives response with embeddings and knowledge graph data (possibly). This will be deployed in docker.

SO now start prepping the folder and folder structure and infra for Fast API and this requirements. The below plan is just initial plan but is still prone to develop a lot

For your context:
/reference_code folder has the code that has been pulled from previous prototype demo, that has the file processing pipeline. But we are totally updgrading that pipeline. 
/pre-tests has the test files that have the testst done on differetn libraries and frameworks on the approach we will be following now. 
The Screenshot attached has the planned flow for this project. 

So you can take the /reference_code with a pinch of salt and /pre-tests are the core concepts that will be scaling and implementing properly with all file type. 

# Plan and Flow

Here is the basic Flow and plan :

📥 Input Sources

The pipeline accepts the following types of inputs:
	•	Document Files:
	•	PDFs, Word Docs, Excel Sheets
	•	CSV, JPEG, MP4, etc.
	•	Raw Text Files
	•	Web Links

⸻

🔄 File Indexer API

All incoming files and links are processed through the File Indexer API. This API is responsible for:
	•	Receiving files or links
	•	Enqueuing them for processing
	•	Storing associated metadata

⸻

📦 Queue and Temporary Storage

After being received by the API:
	•	Files are placed in a processing queue
	•	Raw text or webhook data is stored with metadata in the queue
	•	Files are uploaded to temporary object storage (e.g., S3 or MinIO)

⸻

🧠 Input Type Detection

The Input Type Detector determines the nature of the input and routes it appropriately:
	•	Document Files
	•	Raw Text
	•	Web Links
	•	Audio Files
	•	Video Files

⸻

📄 Processing Document Files
	1.	Convert to Images: Each page or element is transformed into image format.
	2.	Extract Text: Text is extracted from both the original document and the image representation.
	3.	Captioned Images: Images are captioned using vision models.
	4.	Multimodal Embeddings: Both text and image data are passed to a multimodal embedding model.
	5.	Vector Database: Resulting embeddings are stored in a vector DB for retrieval.

⸻

✍️ Processing Raw Text
	1.	Markdown Conversion: Raw text is converted to markdown.
	2.	Extract Text: Directly use the text or enhanced markdown for further analysis.
	3.	LLM Processing:
	•	Ollama LLM is used to analyze or enrich the markdown content.

⸻

🌐 Processing Web Links
	•	Check if YouTube URL:
	•	Yes:
	•	Use Mark it Down → Extract HTML Content → Use Ollama VLM
	•	No:
	•	Browse Page → Extract HTML Content → Use Ollama VLM

⸻

🔊 Processing Audio Files
	•	Decision node: Speech?
	•	If Yes:
	•	Use Mark it Down → Extract Transcription
	•	If No:
	•	Yet to plan

⸻

🎥 Processing Video Files
	•	yet to plan

⸻

🗃 Metadata Schema

Each item processed stores metadata in the following schema:
	1.	Source – The origin of the file or link
  2.  Data - The actual data (Byte IO in case of a file, JSON String in case of Weblink or RAW text)
	3.	Metadata – Metadata for the data, can be different for different data sources and input types. Like file can have file name, mime type, etc etc. RAW Text can have context, to, from, etc etc
	3.	Indexing Date and Time – Date.Now -> This is just for the versioning purpose that will store the current time and date at which the data has been sent for indexing.

# Yet to be plannned

1. How will the embedded data will reach vector db? For this we are yet to write API for vector DB. So this API will be calling the vector db API to store the embeddings
2. Are we planning to build knowlegde graphs? No plan as of now. But will plan once the basic pipeline is done.
3. Audio and Video Processing

# API Design

openapi: 3.0.3
info:
  title: File Indexer API
  version: 1.2.0
  description: API for ingesting files, raw text, and URLs, with support for checking processing status.

servers:
  - url: https://api.example.com/v1

paths:
  /ingest/file:
    post:
      summary: Upload a file for indexing
      requestBody:
        required: true
        content:
          multipart/form-data:
            schema:
              type: object
              required:
                - file
                - source
              properties:
                file:
                  type: string
                  format: binary
                  description: The file to be processed
                source:
                  type: string
                  description: The source of the file
                metadata:
                  type: string
                  description: JSON string containing metadata about the file
      responses:
        '202':
          description: File accepted for processing
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/IngestResponse'
        '400':
          description: Invalid file format or request

  /ingest/url:
    post:
      summary: Submit a web link for indexing
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/UrlIngestRequest'
      responses:
        '202':
          description: URL accepted for processing
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/IngestResponse'
        '400':
          description: Invalid URL or request format

  /ingest/raw-text:
    post:
      summary: Submit raw text for indexing
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/RawTextIngestRequest'
      responses:
        '202':
          description: Raw text accepted for processing
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/IngestResponse'
        '400':
          description: Invalid input

  /status/{item_id}:
    get:
      summary: Get processing status of a submitted item
      parameters:
        - name: item_id
          in: path
          required: true
          schema:
            type: string
          description: Unique identifier of the submitted item
      responses:
        '200':
          description: Current status of the item
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/StatusResponse'
        '404':
          description: Submission ID not found

components:
  schemas:
    IngestResponse:
      type: object
      properties:
        id:
          type: string
          description: Unique identifier for tracking the submission
        message:
          type: string
          description: Status message
      required:
        - id
        - message
      
    UrlIngestRequest:
      type: object
      properties:
        url:
          type: string
          format: uri
          description: The URL to be processed
        source:
          type: string
          description: The source of the URL
        metadata:
          type: object
          additionalProperties: true
          description: Additional metadata about the URL
      required:
        - url
        - source
        
    RawTextIngestRequest:
      type: object
      properties:
        text:
          type: string
          description: The raw text content to be processed
        source:
          type: string
          description: The source of the text
        metadata:
          type: object
          additionalProperties: true
          description: Additional metadata about the text
      required:
        - text
        - source
        
    StatusResponse:
      type: object
      properties:
        id:
          type: string
          description: The unique identifier of the submission
        status:
          type: string
          enum: [queued, processing, completed, failed]
          description: Current processing status
        message:
          type: string
          description: Additional status details or progress information
        result:
          type: object
          nullable: true
          description: Processing results if completed
      required:
        - id
        - status
        - message

  securitySchemes:
    ApiKeyAuth:
      type: apiKey
      in: header
      name: X-API-Key

security:
  - ApiKeyAuth: []

# Current Implementation Plan

1. Lets focus only on Documents with Images and text and focus on Video and audio later. But we will be buidling the temp classes for them too. 
2. 

# How are we doing Emedding and LLM usage? (Model Router API)

I have custom built a reverse engineered Open AI like endpoints API that directly works with Openai python library (for text generation and also structured outputs) and for embedding we will desicde if we directly use the openai python library or should cusomtly do the curl commands.

# Queue Handling

The usage for queue will be:
What ever the request we get we will put that in the queue. As the file will always be coming in byte IO and RAW text weill be coming in JSON string and Web Link will be cmoing in JSON string we can direct put them in the queue (in db). Then we will read them one by one the order of there request and process them.

The request payload will have ,

Source -> From which data source the request is coming from
Data -> The actual data (Byte IO in case of a file, JSON String in case of Weblink or RAW text)
Metadata -> Metadata for the data, can be different for different data sources and input types. Like file can have file name, mime type, etc etc. RAW Text can have context, to, from, etc etc
Indexing Date and time -> Date.Now -> This is just for the versioning purpose that will store the current time and date at which the data has been sent for indexing.
One we recevie a request, we will generate unique id for that queue item and add them to queue. We will be returing this id as a response for status checks from cleints.

# Queue Consumer

in app/services

We need to write queue consumer class.

This will be doing the below things:
It will consume the item from the queue and route it to respective processor and accept processed data from the processors.
There will be only three processors, 
1. FIle Processor
2. Raw text Processor
3. URL processor
We will be creating this processor classes later in app/processors
Every process will be taking the data differetnly but giving out the same kinds of return types. There will be two type of return types from the processors.

1. {data:[{"image": Base64 IMage of the page,
"text": "Extracted Text from that page"}],source:, metadata:, timestamp:}
2. {data:["Markdown Text"],source:, metadata:, timestamp:}

"1" will be if the processor detects some images in the data and "2" will be if the processor detects that the input data will only have text data.
Once this  queue consumer class recevied this retuned data from the processors, it will use the model handler to generate embeddings for these. If the data has [{"image": Base6... then it will pass this data to embed_image else it will pass this data to embed_text
Once it gets the embeddings, it will add the embeddings to the returned payload and send it somewhere (yet to be decided)

Here for The failure queue Item, we will create a new db table that infers from queue_item.py table ajust add this insatce to that table if in failure.

# Processors

app/processors

Role of processors:
The take the data and other things from the queue processor and process the data to normalise it to one of the two kinds as mentioned below:

1. [{"image": Base64 IMage of the page,
"text": "Extracted Text from that page"}] -> if the processor detects some images in the data
2. ["Markdown Text"] -> if the processor detects that the input data only has text data

Well the actual thing that the processor return might not only be this but some additional things too. But this is the core job. These processors will use dependencies like MarkitDown, Image Converters to Normalise anything they see to the either one of the above returns

## Raw Text Processor

The RAW Text processor will perform this actions:
1. Take the RAW text that will be in the JSON String
2. Use MarkitDown to convert it to Markdown format
3. Return the markdowned string

## URL Processor

Role of this processor as of now:

1. Check if the URL is a Youtube URL, if yes, direclty pass it to MarkDown Class,
2. If not, as of not throw error that cannot process this URL, we will plan and handle that later

## File Processor

We will be identifying any file as one of the three categories as mentioned below:
1. PDF, Docx, Doc, PPT, etc (The Documets without Structured Data) -> If Text only, then convert it to markdown directly with markdown class. If Has images, convert the whole document to list of images (1 image per page) adn also pass the file to mark it down to extract possible text.
2. CSV, Excel, etc (The Documets with the data already structred) -> Convert to markdown with markdown class
3. Markdown, JSON, Code files (.py, .c, etc) (The Documents that can we processed as it is - No Conversion Required)

The challenges here are:
1. Detecting the file type regardeless of the mime_type mentioned in the metadata
2. Detecting in which of the three initial areas the file will fall into, Hardcoding the formts can be an Idea. We will start witrh a set of  2-3 files in the format for each of the three kinds and expand it later
3. Detecting wehther the file has only text or Image data too? we can do this by reading the file in some way
4. Handling all file types havign a module to convert any of the file type to Images, if images are detected. Here we will be doing 1 page per image. One way of doping this is converting any file to PDF and then converting PDF to images

Other than these challenges, converting the text in it to markdown will be straighforward and is supported by MarkDown Class.

### Implementing a Comprehensive File Processor System
The provided flowchart depicts an intelligent file processing system that categorizes and processes documents based on their type and content. This report outlines how to implement this system using Python libraries and tools, with a focus on accurate file type detection, content analysis, and appropriate conversion methods.

### File Type Detection Approaches
The first critical step in building the file processor is accurate file type detection, regardless of file extensions or metadata.

### Content-Based File Type Detection
Several Python libraries can identify file types by examining file content rather than relying on extensions
Google's Magika represents the cutting edge, using deep learning to achieve ~99% accuracy across 200+ content types. (https://github.com/google/magika)

### File Categorization System
After detection, files are categorized into three main groups:
def categorize_file(mime_type):
    # Documents without structured data
    unstructured_docs = [
        'application/pdf', 
        'application/msword',
        'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
        'application/vnd.ms-powerpoint',
        'application/vnd.openxmlformats-officedocument.presentationml.presentation'
    ]
    
    # Documents with structured data
    structured_docs = [
        'text/csv',
        'application/vnd.ms-excel',
        'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
    ]
    
    # Documents that can be processed as-is
    direct_processing_docs = [
        'text/markdown',
        'application/json',
        'text/plain',
        'text/x-python',
        'text/x-c'
    ]
    
    if mime_type in unstructured_docs:
        return 'unstructured'
    elif mime_type in structured_docs:
        return 'structured'
    elif mime_type in direct_processing_docs:
        return 'direct'
    else:
        return 'unknown'

### Processing Unstructured Documents
Unstructured documents like PDFs and Office files require special handling based on their content (text-only vs. containing images).
For non-PDF documents, Pandoc can first convert them to PDF format

pdf2image can help analyze PDF content

### Converting Documents to Images
The pdf2image library is particularly useful for converting PDFs to images, using poppler's pdftoppm and pdftocairo tools under the hood

### Text Extraction from Documents
Our MarkDown class can do it

### Processing Structured Documents
Our MarkDown class can do it and convert them to Markdown

### Processing Ready-to-Use Documents
For documents that can be processed as-is (markdown, JSON, code files), minimal processing is needed
Python's Markdown library provides support for processing and extending markdown content with various features. google's Magicka can detect what code is it by reading the content.    

# Coding Style and Tech stack

1. It should be completelely modular, Object oriented, well logged
2. No Icons/emoji's in the code or logs
3. As of now we will be sotrng the queue and databse in the same SQlite Db but later upgrading to redis and postgres. So the coding style should be in such a way that with minimal changes we should be able to upgrade.
4. For object storage we will not yet use S3, but use soem temp alternative like how we are using sqlite for postgres aternative but later will be replacing.
5. No Comments or class/methods descriptions should be written.